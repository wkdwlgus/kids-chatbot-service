# from typing import List
# import joblib
# import numpy as np
# from config import settings

# class PCAEmbeddings:
#     """PCA ê¸°ë°˜ ì„ë² ë”© (512ì°¨ì›)"""
    
#     def __init__(self):
#         print(f"PCA ëª¨ë¸ ë¡œë”© ì¤‘: {settings.PCA_MODEL_PATH}")
        
#         # PCA ëª¨ë¸ ë¡œë“œ
#         self.pca = joblib.load(settings.PCA_MODEL_PATH)
#         print(f"âœ… PCA ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {self.pca.n_components_}ì°¨ì›")
        
#         # GPU ì—¬ë¶€ì— ë”°ë¼ ë¶„ê¸°
#         if settings.USE_GPU:
#             print("âš ï¸  GPU ëª¨ë“œ: Mock ì„ë² ë”© ì‚¬ìš© (ê°œë°œìš©)")
#             self.use_mock = True
#         else:
#             print("CPU ëª¨ë“œ: OpenAI ì„ë² ë”© ì‚¬ìš©")
#             from openai import OpenAI
#             self.client = OpenAI(api_key=settings.OPENAI_API_KEY)
#             self.use_mock = False
        
#         print("âœ… ì„ë² ë”© ì¤€ë¹„ ì™„ë£Œ!")
    
#     def _get_mock_embedding(self, text: str) -> np.ndarray:
#         """Mock ì„ë² ë”© ìƒì„± (GPU í™˜ê²½ìš©)"""
#         # í…ìŠ¤íŠ¸ë¥¼ ì‹œë“œë¡œ ì‚¬ìš©í•´ì„œ ì¼ê´€ì„± ìœ ì§€
#         seed = hash(text) % (2**32)
#         np.random.seed(seed)
        
#         # PCA ì…ë ¥ ì°¨ì›ì— ë§ëŠ” ëœë¤ ë²¡í„°
#         mock_embedding = np.random.randn(self.pca.n_features_in_)
        
#         # ì •ê·œí™” (ì‹¤ì œ ì„ë² ë”©ì²˜ëŸ¼ ë³´ì´ê²Œ)
#         mock_embedding = mock_embedding / np.linalg.norm(mock_embedding)
        
#         return mock_embedding
    
#     def _get_openai_embedding(self, text: str) -> np.ndarray:
#         """OpenAI ì„ë² ë”© ìƒì„± (CPU í™˜ê²½ìš©)"""
#         response = self.client.embeddings.create(
#             model="text-embedding-3-large",
#             input=text
#         )
        
#         embeddings = np.array(response.data[0].embedding)
        
#         # ì°¨ì› ë§ì¶”ê¸°
#         if len(embeddings) != self.pca.n_features_in_:
#             if len(embeddings) > self.pca.n_features_in_:
#                 embeddings = embeddings[:self.pca.n_features_in_]
#             else:
#                 embeddings = np.pad(
#                     embeddings, 
#                     (0, self.pca.n_features_in_ - len(embeddings))
#                 )
        
#         return embeddings
    
#     def embed_query(self, text: str) -> List[float]:
#         """ë‹¨ì¼ ì¿¼ë¦¬ ì„ë² ë”©"""
#         # GPU/CPUì— ë”°ë¼ ë‹¤ë¥¸ ì„ë² ë”© ì‚¬ìš©
#         if self.use_mock:
#             embeddings = self._get_mock_embedding(text)
#         else:
#             embeddings = self._get_openai_embedding(text)
        
#         # PCA ë³€í™˜
#         pca_embedding = self.pca.transform([embeddings])[0]
        
#         return pca_embedding.tolist()
    
#     def embed_documents(self, texts: List[str]) -> List[List[float]]:
#         """ì—¬ëŸ¬ ë¬¸ì„œ ì„ë² ë”©"""
#         return [self.embed_query(text) for text in texts]

# # ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
# pca_embeddings = PCAEmbeddings()
from typing import List
import joblib
import numpy as np
import torch
from transformers import AutoTokenizer, AutoModel
from config import settings

class PCAEmbeddings:
    """
    Alibaba GTE 7B + PCA ì„ë² ë”© (512ì°¨ì›)
    - ë²¡í„° DB ìƒì„± ì‹œì™€ ë™ì¼í•œ íŒŒì´í”„ë¼ì¸
    """
    
    def __init__(self):
        print("="*70)
        print("ğŸš€ PCA ì„ë² ë”© ì‹œìŠ¤í…œ ì´ˆê¸°í™”")
        print("="*70)
        
        # 1. PCA ëª¨ë¸ ë¡œë“œ
        print(f"ğŸ“¥ PCA ëª¨ë¸ ë¡œë”©: {settings.PCA_MODEL_PATH}")
        self.pca = joblib.load(settings.PCA_MODEL_PATH)
        print(f"âœ… PCA ë¡œë“œ ì™„ë£Œ!")
        print(f"   ì…ë ¥ ì°¨ì›: {self.pca.n_features_in_}")
        print(f"   ì¶œë ¥ ì°¨ì›: {self.pca.n_components_}")
        
        # 2. ë””ë°”ì´ìŠ¤ ì„¤ì •
        self.device = "cuda" if (settings.USE_GPU and torch.cuda.is_available()) else "cpu"
        print(f"ğŸ“± Device: {self.device}")
        
        # 3. Alibaba GTE ëª¨ë¸ ë¡œë“œ
        print(f"ğŸ“¥ ì„ë² ë”© ëª¨ë¸ ë¡œë”©: {settings.EMBEDDING_MODEL}")
        print(f"âš ï¸  7B ëª¨ë¸ ë¡œë”© ì¤‘... ì‹œê°„ì´ ê±¸ë¦½ë‹ˆë‹¤")
        
        self.tokenizer = AutoTokenizer.from_pretrained(settings.EMBEDDING_MODEL)
        self.model = AutoModel.from_pretrained(
            settings.EMBEDDING_MODEL,
            torch_dtype=torch.float16 if self.device == "cuda" else torch.float32,
            device_map="auto" if self.device == "cuda" else None
        )
        
        if self.device == "cpu":
            self.model = self.model.to(self.device)
        
        self.model.eval()
        print(f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")
        
        print("="*70)
        print("âœ… ì´ˆê¸°í™” ì™„ë£Œ!")
        print("="*70)
    
    def _mean_pooling(self, model_output, attention_mask):
        """Mean Pooling - ë¬¸ì¥ ì„ë² ë”© ìƒì„±"""
        token_embeddings = model_output.last_hidden_state
        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()
        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)
    
    def _get_gte_embedding(self, text: str) -> np.ndarray:
        """
        Alibaba GTE ì„ë² ë”© ìƒì„±
        - ë²¡í„° DB ìƒì„± ì‹œì™€ ë™ì¼í•œ ë°©ì‹
        """
        # í† í¬ë‚˜ì´ì§•
        inputs = self.tokenizer(
            text,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=512
        ).to(self.device)
        
        # ì„ë² ë”© ìƒì„±
        with torch.no_grad():
            outputs = self.model(**inputs)
            embeddings = self._mean_pooling(outputs, inputs['attention_mask'])
        
        # numpyë¡œ ë³€í™˜
        embedding = embeddings.cpu().numpy()[0]
        
        # ì •ê·œí™”
        embedding = embedding / np.linalg.norm(embedding)
        
        return embedding
    
    def _adjust_dimension(self, embedding: np.ndarray) -> np.ndarray:
        """
        ì„ë² ë”© ì°¨ì›ì„ PCA ì…ë ¥ ì°¨ì›ì— ë§ì¶¤
        """
        target_dim = self.pca.n_features_in_
        current_dim = len(embedding)
        
        if current_dim == target_dim:
            return embedding
        
        if current_dim > target_dim:
            # ìë¥´ê¸°
            return embedding[:target_dim]
        else:
            # íŒ¨ë”©
            return np.pad(
                embedding,
                (0, target_dim - current_dim),
                mode='constant'
            )
    
    def embed_query(self, text: str) -> List[float]:
        """
        ë‹¨ì¼ ì¿¼ë¦¬ ì„ë² ë”©
        
        Args:
            text: ì¿¼ë¦¬ í…ìŠ¤íŠ¸ (ì˜ˆ: "ë¶€ì‚° ì‹¤ë‚´ ë†€ì´í„°")
        
        Returns:
            PCA ë³€í™˜ëœ 512ì°¨ì› ë²¡í„°
        """
        # 1. Alibaba GTE ì„ë² ë”© (4096ì°¨ì›)
        embedding = self._get_gte_embedding(text)
        
        # 2. ì°¨ì› ì¡°ì •
        embedding = self._adjust_dimension(embedding)
        
        # 3. PCA ë³€í™˜ (512ì°¨ì›)
        pca_embedding = self.pca.transform([embedding])[0]
        
        return pca_embedding.tolist()
    
    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        """
        ì—¬ëŸ¬ ë¬¸ì„œ ì„ë² ë”©
        
        Args:
            texts: ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
        
        Returns:
            PCA ë³€í™˜ëœ 512ì°¨ì› ë²¡í„° ë¦¬ìŠ¤íŠ¸
        """
        embeddings = []
        
        for i, text in enumerate(texts):
            if (i + 1) % 100 == 0:
                print(f"   ì„ë² ë”© ì§„í–‰: {i+1}/{len(texts)}")
            
            embedding = self._get_gte_embedding(text)
            embeddings.append(embedding)
        
        embeddings = np.array(embeddings)
        
        # ì°¨ì› ì¡°ì •
        adjusted_embeddings = []
        for emb in embeddings:
            adjusted_embeddings.append(self._adjust_dimension(emb))
        adjusted_embeddings = np.array(adjusted_embeddings)
        
        # PCA ë³€í™˜
        pca_embeddings = self.pca.transform(adjusted_embeddings)
        
        return pca_embeddings.tolist()

# ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
pca_embeddings = PCAEmbeddings()