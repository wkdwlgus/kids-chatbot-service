from langchain_openai import ChatOpenAI
from langchain_community.llms import HuggingFacePipeline
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
from config import settings  # app/ ê°€ ë£¨íŠ¸ì´ë¯€ë¡œ

def get_llm():
    """GPU ì—¬ë¶€ì— ë”°ë¼ LLM ë°˜í™˜"""
    if settings.USE_GPU:
        print("ğŸŒŸ GPU ì‚¬ìš© - HuggingFacePipeline ë¡œì»¬ ëª¨ë¸ ë¡œë“œ")
        # QWEN ë¡œì»¬ ëª¨ë¸
        tokenizer = AutoTokenizer.from_pretrained(settings.QWEN_MODEL_PATH)
        model = AutoModelForCausalLM.from_pretrained(
            settings.QWEN_MODEL_PATH,
            device_map="auto",
            torch_dtype="auto"
        )
        pipe = pipeline(
            "text-generation",
            model=model,
            tokenizer=tokenizer,
            max_new_tokens=512,
            temperature=0.7,
        )
        return HuggingFacePipeline(pipeline=pipe)
    else:
        print("ğŸŒŸ CPU ì‚¬ìš© - OpenAI Chat ëª¨ë¸ ë¡œë“œ")
        # OpenAI API
        return ChatOpenAI(
            model="gpt-4o-mini",
            temperature=0.7,
            openai_api_key=settings.OPENAI_API_KEY
        )