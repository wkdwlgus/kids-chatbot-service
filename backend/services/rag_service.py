"""
RAG Service - ë¡œì»¬ ëª¨ë¸ ê¸°ë°˜

VectorClient ê¸°ë°˜ì˜ ê³ ë„í™”ëœ ê²€ìƒ‰ ì„œë¹„ìŠ¤
- í¬ë¡œìŠ¤ì¸ì½”ë” ë¦¬ëž­í‚¹ (sentence-transformers)
- MMR ë‹¤ì–‘ì„± í•„í„°ë§
- ë©€í‹°ì¿¼ë¦¬ í™•ìž¥ (ë¡œì»¬ LLM)
"""

from typing import List, Dict, Any, Optional
from utils.vector_client import get_vector_client
from utils.config import get_settings
from utils.logger import logger
import os


class RAGService:
    """
    ê³ ë„í™”ëœ RAG ê²€ìƒ‰ ì„œë¹„ìŠ¤ (ë¡œì»¬ ëª¨ë¸ ê¸°ë°˜)
    """
    
    def __init__(self):
        self.settings = get_settings()
        self.vector_client = get_vector_client()
        self._cross_encoder = None
        self._llm_model = None
        self._is_gpu_environment = self._detect_gpu_environment()
        
        # GPU í™˜ê²½ì—ì„œë§Œ ëª¨ë¸ ë¡œë“œ
        if self._is_gpu_environment:
            self._load_models()
    
    def _detect_gpu_environment(self) -> bool:
        """GPU í™˜ê²½ ê°ì§€"""
        try:
            # ì½”ëž© í™˜ê²½ ì²´í¬
            if 'COLAB_RELEASE_TAG' in os.environ:
                return True
            
            # GPU ê°€ìš©ì„± ì²´í¬
            import torch
            return torch.cuda.is_available()
        except ImportError:
            return False
    
    def _load_models(self):
        """GPU í™˜ê²½ì—ì„œ ëª¨ë¸ ë¡œë“œ"""
        try:
            from sentence_transformers import CrossEncoder
            
            logger.info("ðŸ”„ í¬ë¡œìŠ¤ì¸ì½”ë” ëª¨ë¸ ë¡œë”©...")
            self._cross_encoder = CrossEncoder(
                self.settings.RERANKER_MODEL,
                device='cuda' if self._is_gpu_environment else 'cpu'
            )
            logger.info("âœ… í¬ë¡œìŠ¤ì¸ì½”ë” ë¡œë“œ ì™„ë£Œ")
            
            # TODO: LLM ëª¨ë¸ ë¡œë”© (ì½”ëž©ì—ì„œ êµ¬í˜„ ì˜ˆì •)
            # self._llm_model = AutoModelForCausalLM.from_pretrained(...)
            
        except Exception as e:
            logger.error(f"ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            self._cross_encoder = None
    
    def search_and_rerank(
        self,
        query: str,
        top_k: int = None,
        filters: Optional[Dict[str, Any]] = None,
        use_multi_query: bool = True,
        use_mmr: bool = True
    ) -> List[Dict[str, Any]]:
        """ì™„ì „í•œ RAG ê²€ìƒ‰ íŒŒì´í”„ë¼ì¸"""
        try:
            top_k = top_k or self.settings.MMR_TOP_K
            
            logger.info(f"ðŸ” RAG ê²€ìƒ‰: '{query}' (í™˜ê²½: {'GPU' if self._is_gpu_environment else 'Mock'})")
            
            # 1. ë©€í‹°ì¿¼ë¦¬ í™•ìž¥ (GPU í™˜ê²½ì—ì„œë§Œ)
            queries = self._expand_query_multi(query) if use_multi_query else [query]
            
            # 2. ì´ˆê¸° ê²€ìƒ‰ (ë§Žì€ ìˆ˜)
            all_results = []
            for q in queries:
                results = self.vector_client.search(
                    q, 
                    n_results=self.settings.TOP_K,
                    where=filters
                )
                if results['documents']:
                    all_results.extend(self._format_search_results(results))
            
            # ì¤‘ë³µ ì œê±°
            unique_results = self._remove_duplicates(all_results)
            
            # 3. í¬ë¡œìŠ¤ì¸ì½”ë” ë¦¬ëž­í‚¹
            if self._cross_encoder and self._is_gpu_environment:
                reranked_docs = self._cross_encoder_rerank(query, unique_results)
            else:
                logger.info("ðŸ”„ Mock ë¦¬ëž­í‚¹ ì‚¬ìš© (GPU í™˜ê²½ ì•„ë‹˜)")
                reranked_docs = unique_results[:self.settings.RERANK_TOP_K]
            
            # 4. MMR ë‹¤ì–‘ì„± í•„í„°ë§
            if use_mmr:
                final_docs = self._mmr_filtering(query, reranked_docs)
            else:
                final_docs = reranked_docs[:top_k]
            
            logger.info(f"âœ… RAG ê²€ìƒ‰ ì™„ë£Œ: {len(final_docs)}ê°œ ê²°ê³¼")
            return final_docs
            
        except Exception as e:
            logger.error(f"RAG ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []
    
    def _expand_query_multi(self, query: str) -> List[str]:
        """ë©€í‹°ì¿¼ë¦¬ í™•ìž¥"""
        if not self._is_gpu_environment:
            logger.info("ðŸ”„ Mock ì¿¼ë¦¬ í™•ìž¥")
            return [query]  # Mock: ì›ë³¸ ì¿¼ë¦¬ë§Œ ë°˜í™˜
        
        # TODO: GPU í™˜ê²½ì—ì„œ LLM ê¸°ë°˜ ì¿¼ë¦¬ í™•ìž¥
        # ì½”ëž©ì—ì„œ êµ¬í˜„ ì˜ˆì •
        logger.info("ðŸ”„ LLM ì¿¼ë¦¬ í™•ìž¥ (êµ¬í˜„ ì˜ˆì •)")
        return [query]
    
    def _cross_encoder_rerank(self, query: str, docs: List[Dict]) -> List[Dict]:
        """í¬ë¡œìŠ¤ì¸ì½”ë” ë¦¬ëž­í‚¹"""
        if not self._cross_encoder:
            return docs
        
        try:
            # ì¿¼ë¦¬-ë¬¸ì„œ ìŒ ìƒì„±
            pairs = [(query, doc['content']) for doc in docs]
            
            # í¬ë¡œìŠ¤ì¸ì½”ë” ì ìˆ˜ ê³„ì‚°
            scores = self._cross_encoder.predict(pairs)
            
            # ì ìˆ˜ì™€ ë¬¸ì„œ ë§¤ì¹­
            scored_docs = []
            for doc, score in zip(docs, scores):
                if score >= self.settings.SIMILARITY_THRESHOLD:
                    doc['rerank_score'] = float(score)
                    scored_docs.append(doc)
            
            # ì ìˆ˜ ê¸°ì¤€ ì •ë ¬
            scored_docs.sort(key=lambda x: x['rerank_score'], reverse=True)
            
            return scored_docs[:self.settings.RERANK_TOP_K]
            
        except Exception as e:
            logger.error(f"ë¦¬ëž­í‚¹ ì‹¤íŒ¨: {e}")
            return docs
    
    def _mmr_filtering(self, query: str, docs: List[Dict]) -> List[Dict]:
        """MMR ë‹¤ì–‘ì„± í•„í„°ë§ (TODO: êµ¬í˜„)"""
        # TODO: Maximum Marginal Relevance êµ¬í˜„
        # í˜„ìž¬ëŠ” ìƒìœ„ Nê°œë§Œ ë°˜í™˜
        return docs[:self.settings.MMR_TOP_K]
    
    def _format_search_results(self, results: Dict[str, Any]) -> List[Dict[str, Any]]:
        """ê²€ìƒ‰ ê²°ê³¼ë¥¼ í‘œì¤€ í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
        formatted = []
        for doc, meta, dist in zip(
            results['documents'],
            results['metadatas'],
            results['distances']
        ):
            formatted.append({
                'content': doc,
                'metadata': meta,
                'distance': dist,
                'similarity': 1 - dist  # ê±°ë¦¬ë¥¼ ìœ ì‚¬ë„ë¡œ ë³€í™˜
            })
        return formatted
    
    def _remove_duplicates(self, docs: List[Dict]) -> List[Dict]:
        """ì¤‘ë³µ ë¬¸ì„œ ì œê±° (facility_name ê¸°ì¤€)"""
        seen_names = set()
        unique_docs = []
        
        for doc in docs:
            name = doc['metadata'].get('facility_name', '')
            if name and name not in seen_names:
                seen_names.add(name)
                unique_docs.append(doc)
        
        return unique_docs
    
    
    def has_location_data(self, docs: List[Dict[str, Any]]) -> bool:
        """ê²€ìƒ‰ ê²°ê³¼ì— ìœ„ì¹˜ ì •ë³´ê°€ ìžˆëŠ”ì§€ í™•ì¸"""
        for doc in docs[:3]:  # ìƒìœ„ 3ê°œë§Œ í™•ì¸
            meta = doc.get('metadata', {})
            if meta.get('latitude') and meta.get('longitude'):
                return True
        return False
    
    def get_location_summary(self, docs: List[Dict[str, Any]]) -> str:
        """ìœ„ì¹˜ ê¸°ë°˜ ê²€ìƒ‰ ê²°ê³¼ ìš”ì•½"""
        if not docs:
            return "ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."
        
        locations = []
        for doc in docs[:3]:
            meta = doc.get('metadata', {})
            name = meta.get('facility_name')
            region = f"{meta.get('region_city', '')} {meta.get('region_gu', '')}".strip()
            
            if name:
                locations.append(f"{name} ({region})" if region else name)
        
        if locations:
            return f"ë‹¤ìŒ ìž¥ì†Œë“¤ì„ ì¶”ì²œí•´ë“œë ¤ìš”: {', '.join(locations)}"
        else:
            return "ê´€ë ¨ ì‹œì„¤ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤."


# ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
_rag_service_instance = None

def get_rag_service() -> RAGService:
    """RAG Service ì‹±ê¸€í†¤ ë°˜í™˜"""
    global _rag_service_instance
    if _rag_service_instance is None:
        _rag_service_instance = RAGService()
    return _rag_service_instance