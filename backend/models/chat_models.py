from langchain_openai import ChatOpenAI
from langchain_community.llms import HuggingFacePipeline
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, BitsAndBytesConfig
import torch
from config import settings

def get_llm():
    """GPU ì—¬ë¶€ì— ë”°ë¼ LLM ë°˜í™˜"""
    if settings.USE_GPU and torch.cuda.is_available():
        print("="*70)
        print("ğŸš€ Qwen 2.5 7B ëª¨ë¸ ë¡œë”© ì¤‘...")
        print("="*70)
        
        tokenizer = AutoTokenizer.from_pretrained(
            settings.GENERATION_MODEL,
            trust_remote_code=True
        )
        
        # 4-bit quantization (8GB GPUìš©)
        print("ğŸ”§ 4-bit quantization ì ìš©")
        quantization_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_compute_dtype=torch.float16,
            bnb_4bit_use_double_quant=True,
            bnb_4bit_quant_type="nf4"
        )
        
        model = AutoModelForCausalLM.from_pretrained(
            settings.GENERATION_MODEL,
            quantization_config=quantization_config,
            device_map="auto",
            trust_remote_code=True
        )
        
        print("âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")
        
        pipe = pipeline(
            "text-generation",
            model=model,
            tokenizer=tokenizer,
            max_new_tokens=512,
            temperature=0.7,
            do_sample=True,
            top_p=0.9
        )
        
        print("âœ… íŒŒì´í”„ë¼ì¸ ì¤€ë¹„ ì™„ë£Œ!")
        print("="*70)
        
        return HuggingFacePipeline(pipeline=pipe)
    
    else:
        print("âš ï¸  CPU ëª¨ë“œ: OpenAI API ì‚¬ìš©")
        return ChatOpenAI(
            model="gpt-4o-mini",
            temperature=0.7,
            openai_api_key=settings.OPENAI_API_KEY
        )